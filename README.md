# For what?


When you copy text from a journal to cite it , you will have to delete newlines,
page numbers footnotes, if there is something going over the page breaks. It's more complicated with
 the layout tricks, that are used every where: columns, pictures, captions, tables, headers.
 Getting the actual text in a layouted document is very different from getting all the text.

 It is not perfect, but the concept works somehow. The folowing link opens a html-document with a document, that looks the same like the original pdf (made with pdf2htmlEX), but the text is colored to represent the recognized layout.
 (red: not to be included in the actual text, light green: column 1, dark green: column 2, blue: column 3)

[resulting document](https://github.com/c0ntradicti0n/LayoutEagle/raw/master/show/col2.pdf)


# Why?

The aim of this project is especially to get text from column layouts.

So this is another attempt, trying with a supervised deep learning approach.

A neuronal network is fed with features of the texboxes in the pdf.
The training data is automatically collected and produced: We use free publications with
 the LateX source files and replace all the text in real journal articles by the labels,
 that shoud be learned by the neuronal network.
 .
(These LateX documents are scraped from the arxiv.org website)
It's learning curve:

![learning curve](https://github.com/c0ntradicti0n/LayoutEagle/raw/master/accuracy_epocs2.png)

But the result is not a total succes yet:
![resulting prediction](https://github.com/c0ntradicti0n/LayoutEagle/raw/master/show/2col.html)

The design of this project follows a "new design pattern", called here "path-ant". It's named like this because ants
like to bring things from one location to another, leaving it there, when they don't know where to bring it further
and go along to find something new to bring. Alike in the software of this repository, all classes are registered as
"converters", because they transform one kind of data to another, without knowing to what end. And the full conversion
pipelines are made from a graph of the class registry by looking for the shortest path:

![path-ants graph](https://github.com/c0ntradicti0n/LayoutEagle/raw/master/pathant.png)

(One of the colored paths is the pipeline to generate the model data and the other one is to get some predicted output-
documents, to see the results.)

If you want to write your own models and work with the results here, you could easily also register your modules in the
same apparatus of the path-ant and reuse its autogenerated pipelines.


# Installation

upgrade pip(>20) and setuptools(>46):
```
pip install --upgrade pip setuptools
```

Clone this repository with git and navigate into the repo.

Additional you need to install

Graphiviz:

sudo apt-get install graphviz libgraphviz-dev graphviz-dev pkg-config

* Pastel:

wget "https://github.com/sharkdp/pastel/releases/download/v0.7.1/pastel_0.7.1_amd64.deb"
sudo dpkg -i pastel_0.7.1_amd64.deb

* PDF2HTMLEX:

simple way here is to go with the "releases" on the github project page, else compile it yourself compiling
also `fontforge` and `poppler`, but it's important to take the right versions.
https://github.com/pdf2htmlEX/pdf2htmlEX

* Full LateX:

sudo apt-get install texlive-full texlive-publishers texlive-science texlive-pstricks texlive-pictures  texlive-latex-extra

To install itself:

```
pip install git+git://github.com/c0ntradicti0n/LayoutEagle.git@master#egg=layoute --upgrade
```


# Layout recognition on PDF data

To obtain a model, start the pipeline defined in 'make_model,py':
```
from layouteagle.LayoutEagle import LayoutEagle
layouteagle = LayoutEagle("dir_to_put_model_to")
layouteagle.make_model(n=150)
```



