# Why?

Maybe you know the problem of copying text from a journal to cite it one day. You will have to delete newlines,
page numbers footnotes, if there is something going over the page breaks. This is also a bigger problem for
natural language processing with scientific information, because one has to extract the text from the articles.
And this task gets the more difficult, the more one thinks of all the columns, pictures, captions, tables, that are
thrown between the text.

There are some other projects, that try to get text and other data from PDF-files.
There are rule-based approaches, that make mistakes due to problems, that arise from defining rules for many different layouts.
And there are recommendations for using deep learning and non-free services for this.

The aim of this project is especially to get text from column layouts.

So this is another attempt, trying with a manifold neuronal network, that is fed with features of the texboxes in the pdf.
The training data is automatically produced, by replacing the text in real journal articles by labels for the text and
recognizing them afterwards as the labels for the texboxes in the pdfs after parsing the LateX documents.
(These LateX documents are scraped from the arxiv.org website)
It achieves around 97.5 % on the validation set, as the learning curve reveals:

![learning curve](https://github.com/c0ntradicti0n/CorpusCookApp/raw/master/accuracy_epocs2.png)

But the result is not such a succes yet:
![resulting prediction](https://github.com/c0ntradicti0n/CorpusCookApp/raw/master/result.png)

The design of this project follows a "new design pattern", called here "path-ant". It's named like this because ants
like to bring things from one location to another, leaving it there, when they don't know where to bring it further
and go along to find something new to bring. Alike in the software of this repository, all classes are registered as
"converters", because they transform one kind of data to another, without knowing to what end. And the full conversion
pipelines are made from a graph of the class registry by looking for the shortest path:

![path-ants graph](https://github.com/c0ntradicti0n/CorpusCookApp/raw/master/pathant.png)

(One of the colored paths is the pipeline to generate the model data and the other one is to get some predicted output-
documents, to see the results.)

If you want to write your own models and work with the results here, you could easily also register your modules in the
same apparatus of the path-ant and reuse its autogenerated pipelines.


# Installation

Clone this repository with git and navigate into the repo.

Additional you need to install

Graphiviz:

sudo apt-get install graphviz libgraphviz-dev graphviz-dev pkg-config

* Pastel:

wget "https://github.com/sharkdp/pastel/releases/download/v0.7.1/pastel_0.7.1_amd64.deb"
sudo dpkg -i pastel_0.7.1_amd64.deb

* PDF2HTMLEX:

simple way here is to go with the "releases" on the github project page, else compile it yourself compiling
also `fontforge` and `poppler`, but it's important to take the right versions.
https://github.com/pdf2htmlEX/pdf2htmlEX

* Full LateX:

sudo apt-get install texlive-full texlive-publishers texlive-science texlive-pstricks texlive-pictures  texlive-latex-extra

To install itself:

```
pip install git+git://github.com/c0ntradicti0n/LayoutEagle.git@master#egg=layoute --upgrade
```


# Layout recognition on PDF data

To obtain a model, start the pipeline defined in 'make_model,py':
```
from layouteagle.LayoutEagle import LayoutEagle
layouteagle = LayoutEagle("dir_to_put_model_to")
layouteagle.make_model(n=150)
```



